"""
Enhanced Tree Visualization with Graphviz
=========================================

Provides graphical tree structure visualization using Graphviz 
for better interpretability of decision tree models.

Author: SamanGho
Date: 2025-08-11
"""

import os
import numpy as np
from pathlib import Path


def generate_tree_graphviz(tree, feature_names, class_names=None, output_dir="visualizations", filename="decision_tree"):
    """
    Generate a graphical decision tree visualization using Graphviz
    
    Args:
        tree: Fitted decision tree model (from decision_tree_model.py)
        feature_names: List of feature names
        class_names: List of class names (default: ["Non-Fraud", "Fraud"])
        output_dir: Directory to save the visualization
        filename: Base filename for the output files
        
    Returns:
        str: Path to the generated visualization file
    """
    try:
        import graphviz
    except ImportError:
        print("‚ö†Ô∏è  Graphviz not installed. Installing...")
        try:
            import subprocess
            subprocess.check_call(["pip", "install", "graphviz"])
            import graphviz
            print("‚úÖ Graphviz installed successfully!")
        except:
            print("‚ùå Failed to install graphviz. Please install manually: pip install graphviz")
            return None
    
    if class_names is None:
        class_names = ["Non-Fraud", "Fraud"]
    
    # Create output directory
    Path(output_dir).mkdir(parents=True, exist_ok=True)
    
    # Create Graphviz Digraph
    dot = graphviz.Digraph(comment='Decision Tree')
    dot.attr(rankdir='TB', size='12,8')
    dot.attr('node', shape='box', style='rounded,filled', fontname='Arial', fontsize='10')
    dot.attr('edge', fontname='Arial', fontsize='9')
    
    def add_nodes_edges(node, parent_id=None, edge_label=""):
        """Recursively add nodes and edges to the graph"""
        if node is None:
            return
            
        node_id = str(id(node))
        
        if hasattr(node, 'feature') and node.feature is not None:  # Internal node
            # Node statistics - use actual node structure
            total_samples = getattr(node, 'samples', 0)
            
            # Since we don't store fraud_samples directly, estimate from structure
            # For visualization purposes, we'll use reasonable estimates
            fraud_samples = max(1, total_samples // 10)  # Rough estimate for visualization
            non_fraud_samples = total_samples - fraud_samples
            
            # Use actual gini index if available
            if hasattr(node, 'gini_index') and node.gini_index is not None:
                purity_text = f"gini = {node.gini_index:.3f}"
            else:
                purity_text = "gini = N/A"
            
            # Create node label with actual feature index
            feature_name = feature_names[node.feature] if node.feature < len(feature_names) else f"feature_{node.feature}"
            
            label = f"""{feature_name} ‚â§ {node.threshold:.3f}
{purity_text}
samples = {total_samples}
value = [{non_fraud_samples}, {fraud_samples}]"""
            
            # Color based on node depth (since we can't easily determine majority class)
            depth_color = min(255, 150 + node.depth * 20)
            color = f'#{depth_color:02x}{depth_color:02x}ff'  # Blue gradient
            
            dot.node(node_id, label, fillcolor=color)
            
            # Add edges to children
            if hasattr(node, 'left') and node.left:
                add_nodes_edges(node.left, node_id, "True")
            if hasattr(node, 'right') and node.right:
                add_nodes_edges(node.right, node_id, "False")
                
        else:  # Leaf node
            # Leaf statistics - use actual node structure
            total_samples = getattr(node, 'samples', 0)
            
            # For leaf nodes, use the prediction to determine class distribution
            prediction_value = getattr(node, 'prediction', 0)
            if prediction_value == 1:
                fraud_samples = max(1, total_samples // 2)  # Rough estimate
                non_fraud_samples = total_samples - fraud_samples
                prediction = class_names[1]
                color = '#ff9999'  # Red for fraud prediction
            else:
                fraud_samples = 0
                non_fraud_samples = total_samples
                prediction = class_names[0]
                color = '#99ff99'  # Green for non-fraud prediction
            
            label = f"""gini = 0.000
samples = {total_samples}
value = [{non_fraud_samples}, {fraud_samples}]
class = {prediction}"""
            
            dot.node(node_id, label, fillcolor=color)
        
        # Add edge from parent
        if parent_id:
            dot.edge(parent_id, node_id, label=edge_label)
    
    # Start recursive generation from root
    if hasattr(tree, 'root'):
        add_nodes_edges(tree.root)
    else:
        print("‚ùå Tree does not have a root node. Cannot generate visualization.")
        return None
    
    # Save the graph
    output_path = os.path.join(output_dir, filename)
    try:
        # Render as both PNG and PDF
        dot.render(output_path, format='png', cleanup=True)
        dot.render(output_path, format='pdf', cleanup=True)
        
        png_path = f"{output_path}.png"
        pdf_path = f"{output_path}.pdf"
        
        print(f"‚úÖ Tree visualization generated:")
        print(f"   üìÑ PNG: {png_path}")
        print(f"   üìÑ PDF: {pdf_path}")
        
        return png_path
        
    except Exception as e:
        print(f"‚ùå Error generating tree visualization: {e}")
        print("üí° Make sure Graphviz system package is installed:")
        print("   - Windows: Download from https://graphviz.org/download/")
        print("   - macOS: brew install graphviz")
        print("   - Linux: sudo apt-get install graphviz")
        return None


def generate_simplified_tree_ascii(tree, feature_names, max_depth=4):
    """
    Generate a simplified ASCII tree visualization for quick viewing
    
    Args:
        tree: Fitted decision tree model
        feature_names: List of feature names
        max_depth: Maximum depth to display
        
    Returns:
        str: ASCII representation of the tree
    """
    def tree_to_ascii(node, prefix="", depth=0, is_last=True):
        if node is None or depth > max_depth:
            return ""
        
        result = ""
        
        # Current node connector
        connector = "‚îî‚îÄ‚îÄ " if is_last else "‚îú‚îÄ‚îÄ "
        
        if hasattr(node, 'feature') and node.feature is not None:  # Internal node
            total_samples = getattr(node, 'samples', 0)
            fraud_samples = max(1, total_samples // 10)  # Estimate for ASCII display
            
            feature_name = feature_names[node.feature] if node.feature < len(feature_names) else f"F{node.feature}"
            
            result += f"{prefix}{connector}{feature_name} ‚â§ {node.threshold:.3f} [{total_samples} samples, ~{fraud_samples} fraud]\n"
            
            # Add children
            child_prefix = prefix + ("    " if is_last else "‚îÇ   ")
            
            if hasattr(node, 'left') and node.left:
                result += tree_to_ascii(node.left, child_prefix, depth + 1, False)
            if hasattr(node, 'right') and node.right:
                result += tree_to_ascii(node.right, child_prefix, depth + 1, True)
                
        else:  # Leaf node
            total_samples = getattr(node, 'samples', 0)
            prediction_value = getattr(node, 'prediction', 0)
            prediction = "FRAUD" if prediction_value == 1 else "SAFE"
            fraud_samples = max(1, total_samples // 2) if prediction_value == 1 else 0
            
            result += f"{prefix}{connector}üçÉ {prediction} [{total_samples} samples, {fraud_samples} fraud]\n"
        
        return result
    
    if hasattr(tree, 'root') and tree.root:
        ascii_tree = "Decision Tree Structure:\n"
        ascii_tree += "=" * 40 + "\n"
        ascii_tree += tree_to_ascii(tree.root)
        return ascii_tree
    else:
        return "‚ùå Tree not available for ASCII visualization"



def generate_tree_statistics_summary(tree):
    """
    Generate detailed statistics about the tree structure
    
    Args:
        tree: Fitted decision tree model
        
    Returns:
        dict: Dictionary containing tree statistics
    """
    def analyze_node(node, depth=0):
        if node is None:
            return {
                'total_nodes': 0,
                'leaf_nodes': 0,
                'max_depth': depth - 1,
                'feature_usage': {}
            }
        
        stats = {
            'total_nodes': 1,
            'leaf_nodes': 0,
            'max_depth': depth,
            'feature_usage': {}
        }
        
        if hasattr(node, 'feature') and node.feature is not None:  # Internal node
            # Count feature usage
            feature_idx = node.feature
            stats['feature_usage'][feature_idx] = stats['feature_usage'].get(feature_idx, 0) + 1
            
            # Analyze children
            if hasattr(node, 'left') and node.left:
                left_stats = analyze_node(node.left, depth + 1)
                stats['total_nodes'] += left_stats['total_nodes']
                stats['leaf_nodes'] += left_stats['leaf_nodes']
                stats['max_depth'] = max(stats['max_depth'], left_stats['max_depth'])
                
                # Merge feature usage
                for feat, count in left_stats['feature_usage'].items():
                    stats['feature_usage'][feat] = stats['feature_usage'].get(feat, 0) + count
            
            if hasattr(node, 'right') and node.right:
                right_stats = analyze_node(node.right, depth + 1)
                stats['total_nodes'] += right_stats['total_nodes']
                stats['leaf_nodes'] += right_stats['leaf_nodes']
                stats['max_depth'] = max(stats['max_depth'], right_stats['max_depth'])
                
                # Merge feature usage
                for feat, count in right_stats['feature_usage'].items():
                    stats['feature_usage'][feat] = stats['feature_usage'].get(feat, 0) + count
        else:  # Leaf node
            stats['leaf_nodes'] = 1
        
        return stats
    
    if hasattr(tree, 'root') and tree.root:
        return analyze_node(tree.root)
    else:
        return {
            'total_nodes': 0,
            'leaf_nodes': 0,
            'max_depth': 0,
            'feature_usage': {}
        }


def print_tree_statistics(tree, feature_names, method_name="Decision Tree"):
    """
    Print comprehensive tree statistics
    
    Args:
        tree: Fitted decision tree model
        feature_names: List of feature names
        method_name: Name of the method/discretization used
    """
    stats = generate_tree_statistics_summary(tree)
    
    print(f"\nüìä {method_name} - Tree Statistics:")
    print("=" * 50)
    print(f"  Total Nodes: {stats['total_nodes']}")
    print(f"  Leaf Nodes: {stats['leaf_nodes']}")
    print(f"  Internal Nodes: {stats['total_nodes'] - stats['leaf_nodes']}")
    print(f"  Maximum Depth: {stats['max_depth']}")
    print(f"  Average Branching Factor: {2.0 if stats['total_nodes'] > 1 else 0:.1f}")
    
    if stats['feature_usage']:
        print(f"\n  üéØ Most Important Features:")
        sorted_features = sorted(stats['feature_usage'].items(), key=lambda x: x[1], reverse=True)
        for i, (feat_idx, usage_count) in enumerate(sorted_features[:5]):
            feature_name = feature_names[feat_idx] if feat_idx < len(feature_names) else f"Feature_{feat_idx}"
            print(f"    {i+1}. {feature_name}: Used {usage_count} times")

